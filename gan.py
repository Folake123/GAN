# -*- coding: utf-8 -*-
"""ids_gan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18siOMWLLJqIwnFIGUoWznpjnEqoEpa2h
"""

'''import os
os.listdir("drive/")'''

#from google.colab import drive
#drive.mount('/content/drive')

import os
import random
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
#import seaborn as sns
#from tqdm import tqdm_notebook as tqdm
from keras.models import Model
from keras.layers import Input, Reshape
from keras.layers.core import Dense, Activation, Dropout, Flatten
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import UpSampling1D, Conv1D
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam, SGD
from keras.callbacks import TensorBoard
from sklearn.preprocessing import MinMaxScaler

random.seed(42)

lr = 1e-3
epochs = 1500
n_samples = 0
batch_size = 64
noise_dim = 10
n_features = 0

def get_data():
  mydt = pd.read_csv("/scratch/rice/n/nsakhala/unbalanced.csv")
  mydt.columns=['year','month','day','hour','minute','second','duration','source_ip','dest_ip','source_port','dest_port','protocol','flag1','flag2','flag3','flag4','flag5','flag6','fwd','stos','p    kt','byt','label']
  criteria0 =  mydt['label'] == 0  #just attack data
  data0 = mydt[criteria0]
  float_array = data0.values.astype(float)
  float_array = mydt.values.astype(float)
  min_max_scaler = MinMaxScaler()
  scaled_array = min_max_scaler.fit_transform(float_array)
  mydt_normalized = pd.DataFrame(scaled_array)
  X = mydt_normalized.iloc[:, 0:22].values
  n_samples = X.shape[0]
  n_features = X.shape[1]
  return X,n_samples,n_features
	#y = mydt_normalized.iloc[:, 22].values

data,n_samples,n_features = get_data()

#print("values are:  ",data[5])

def sample_data():
  sample = data[np.random.choice(data.shape[0],1000,replace=False)]
	#print ("shape of samples is:",sample.shape)
  return sample,sample.shape[0]

#ax = pd.DataFrame(np.transpose(sample_data(5))).plot()

def get_generative(G_in, dense_dim=128):
  x = Dense(dense_dim)(G_in)
  x = LeakyReLU(alpha=0.2)(x)
  x = Dense(64)(x)
  x = LeakyReLU(alpha=0.2)(x)
  x = Dense(128)(x)
  x = LeakyReLU(alpha=0.2)(x)
  G_out = Dense(n_features, activation='tanh')(x)
  G = Model(G_in, G_out)
  opt = SGD(lr=lr)
  G.compile(loss='binary_crossentropy', optimizer=opt)
  return G, G_out

G_in = Input(shape=[noise_dim,])
G, G_out = get_generative(G_in)
G.summary()

def get_discriminative(D_in, drate=.25, conv_sz=5, leak=.2):
  x = Dense(64)(D_in)
  x = LeakyReLU(alpha=0.2)(x)
  x = Dropout(drate)(x)
  x = Dense(64)(x)
  D_out = Dense(2, activation='sigmoid')(x)
  D = Model(D_in, D_out)
  dopt = Adam(lr=lr)
  D.compile(loss='binary_crossentropy', optimizer=dopt)
  return D, D_out

D_in = Input(shape=[n_features,])
D, D_out = get_discriminative(D_in)
D.summary()

def set_trainability(model, trainable=False):
  model.trainable = trainable
  for layer in model.layers:
    layer.trainable = trainable
    
def make_gan(GAN_in, G, D):
  set_trainability(D, False)
  x = G(GAN_in)
  GAN_out = D(x)
  GAN = Model(GAN_in, GAN_out)
  GAN.compile(loss='binary_crossentropy', optimizer=G.optimizer)
  return GAN, GAN_out

GAN_in = Input([noise_dim,])
GAN, GAN_out = make_gan(GAN_in, G, D)
GAN.summary()

def sample_data_and_gen(G):
  XT,nrows = sample_data()
  XN_noise = np.random.uniform(0, 1, size=[nrows, noise_dim])
  XN = G.predict(XN_noise)
  X = np.concatenate((XT, XN))
  y = np.zeros((2*nrows, 2))
  y[:nrows, 0] = 1
  y[nrows:, 1] = 1
  return X, y,nrows

def pretrain(G, D):
	X, y,_ = sample_data_and_gen(G)
	set_trainability(D, True)
	D.fit(X, y, epochs=1, batch_size=batch_size)

pretrain(G, D)

def sample_noise(G,nrows):
	X = np.random.uniform(0, 1, size=[nrows, noise_dim])
	y = np.zeros((nrows, 2))
	y[:, 1] = 1
	return X, y

def train(GAN, G, D, verbose=False, v_freq=10):
	d_loss = []
	g_loss = []
	e_range = range(epochs)
	if verbose:
		pass
       #e_range = tqdm(e_range)
	for epoch in e_range:
		X, y, nrows = sample_data_and_gen(G)
		set_trainability(D, True)
		d_loss.append(D.train_on_batch(X, y))
       
		X, y = sample_noise(G, nrows)
		set_trainability(D, False)
		g_loss.append(GAN.train_on_batch(X, y))
		if verbose and (epoch + 1) % v_freq == 0:
			print("Epoch #{}: Generative Loss: {}, Discriminative Loss: {}".format(epoch + 1, g_loss[-1], d_loss[-1]))
	return d_loss, g_loss

d_loss, g_loss = train(GAN, G, D, verbose=True)

def plot_data():
	e_range = range(epochs)
	plt.figure(1)  
	plt.plot(d_loss)  
	plt.plot(g_loss)  
	plt.title('losses')  
	plt.ylabel('loss')  
	plt.xlabel('epoch')  
	plt.legend(['discriminator', 'generator'], loc='upper left') 
	plt.savefig('plot.png') 

plot_data()

def save_data(num):
	noise = np.random.uniform(0, 1, size=[num, noise_dim])
	op = G.predict(noise)
	print("output is: ",op.shape)

	np.savetxt("new.csv",op,delimiter=",")

save_data(300)

